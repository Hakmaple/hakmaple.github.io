<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
        
            深度学习：从头构建神经网络 | zer0ptr&#39;s Blog
        
    </title>
    
    <!-- Meta tags -->
    <meta name="description" content="">
    <meta name="author" content="zer0ptr">
    
    <!-- 关键CSS内联 -->
    <style>
        /* 关键渲染路径CSS */
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { 
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6; color: #333; background-color: #fff; font-size: 17px; overflow-x: hidden;
        }
        .container { max-width: 650px; margin: 0 auto; padding: 0 15px; }
        .loading-indicator {
            position: fixed; top: 0; left: 0; width: 100%; height: 3px;
            background: linear-gradient(90deg, #007acc, #4fc3f7);
            transform: translateX(-100%); transition: transform 0.3s ease; z-index: 9999;
        }
        .loading-indicator.active { animation: loadingProgress 1s ease-in-out infinite; }
        @keyframes loadingProgress {
            0% { transform: translateX(-100%); }
            50% { transform: translateX(0%); }
            100% { transform: translateX(100%); }
        }
    </style>
    
    <!-- 异步加载完整CSS -->
    <link rel="preload" href="/css/style.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/css/style.css"></noscript>
    
    <!-- CSS加载polyfill -->
    <script>
        // 确保CSS异步加载在所有浏览器中正常工作
        !function(e){"use strict";var t=function(t,n,r){function o(e){return i.body?e():void setTimeout(function(){o(e)})}function a(){d.addEventListener&&d.removeEventListener("load",a),d.media=r||"all"}var i=e.document,d=i.createElement("link");if(n)d.id=n;d.rel="stylesheet",d.href=t,d.media="only x",o(function(){i.head.appendChild(d)}),setTimeout(a);return d};e.loadCSS=t}(this);
    </script>
    
    <!-- 预加载关键资源 -->
    <link rel="preconnect" href="https://cdnjs.cloudflare.com">
    <link rel="preconnect" href="https://cdn.jsdelivr.net">
    
    <!-- 按需加载Highlight.js CSS -->
    
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
    
    
    <!-- 按需加载MathJax -->
    
        <script>
            window.MathJax = {
                tex: {
                    inlineMath: [['$', '$'], ['\\(', '\\)']],
                    displayMath: [['$$', '$$'], ['\\[', '\\]']],
                    processEscapes: true,
                    processEnvironments: true
                },
                options: {
                    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                }
            };
        </script>
        <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    
    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="/favicon.ico">
    
    <!-- 预加载JavaScript -->
    <link rel="preload" href="/js/search.js" as="script">
    <link rel="preload" href="/js/script.js" as="script">
<meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
<body>
    <!-- 加载指示器 -->
    <div class="loading-indicator" id="loadingIndicator"></div>
    <!-- Top Navigation Bar -->
    
    <nav class="top-navigation">
        <div class="container">
            <div class="nav-brand">
                <a href="/" class="nav-brand-link">
                    zer0ptr&#39;s blog
                </a>
            </div>
            <ul class="top-nav-list">
                
                    <li class="top-nav-item">
                        <a href="/archives" class="top-nav-link">
                            Archives
                        </a>
                    </li>
                
                    <li class="top-nav-item">
                        <a href="/categories" class="top-nav-link">
                            Categories
                        </a>
                    </li>
                
                    <li class="top-nav-item">
                        <a href="/tags" class="top-nav-link">
                            Tags
                        </a>
                    </li>
                
                    <li class="top-nav-item">
                        <a href="/about" class="top-nav-link">
                            About
                        </a>
                    </li>
                
                <li class="top-nav-item">
                    <a href="https://github.com" class="top-nav-link" target="_blank">
                        <svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor">
                            <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/>
                        </svg>
                    </a>
                </li>
                <li class="top-nav-item">
                    <a href="#" class="top-nav-link search-toggle">
                        <svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor">
                            <path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"/>
                        </svg>
                    </a>
                </li>
            </ul>
        </div>
    </nav>
    
    
    <div class="container">
        <!-- Header -->
        <header class="header">
            <div class="header-content">
                <h1 class="site-title">
                    <a href="/">
                        zer0ptr&#39;s blog
                    </a>
                </h1>
                
                    <p class="site-subtitle">
                        Walking on my way🧐.
                    </p>
                
            </div>
        </header>

        <!-- Main Content -->
        <main class="main-content page-transition" id="mainContent">
            <article class="post">
    <!-- Post Header -->
    <header class="post-header">
        
            <div class="post-meta">
                
                    <a href="/categories/AI/" class="category-link">
                        AI
                    </a>
                
            </div>
        
        
        <h1 class="post-title">深度学习：从头构建神经网络</h1>
        
        <div class="post-info">
            <time class="post-date" datetime="2025-07-19T09:46:01.000Z">
                July 19, 2025
            </time>
            
            
            
            
        </div>
        
        
            <div class="post-tags">
                
                    <a href="/tags/AI/" class="tag-link">
                        #AI
                    </a>
                
                    <a href="/tags/Deep-Learning/" class="tag-link">
                        #Deep Learning
                    </a>
                
            </div>
        
    </header>
    
    <!-- Post Content -->
    <div class="post-content">
        <h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>本文旨在使用PyTorch构建并训练一个最简单的神经网络，无需添加任何花哨的层或依赖包。</p>
<p>该模型将足够简单，大家都能使用CPU或GPU来构建和训练。</p>
<p>这个模型虽然简单，但包含了当前诸如LLM和Stable Diffusions等大型模型所拥有的所有基本元素。</p>
<h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>假设我们要训练一个具有四个权重并能输出一个数字结果的模型，如下所示：</p>
<p>$ y &#x3D; w_1 * x_1 + w_2 * x_2 + w_3 * x_3 + w_4 * x_4 $</p>
<p>我们先生成以下训练数据，假设权重值为 [2,3,4,7]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">w_list = np.array([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">7</span>])</span><br></pre></td></tr></table></figure>

<p>我们的模型将会用于预测权重列表，因此再生成一些训练数据后我们假装我们不知道这些权重的值</p>
<p>之后我们创建10组输入样本数据——x_sample，每组x_sample是一个包含4个元素的数组，与权重的长度相同</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generate 10 random input samples (each with same length as w_list)</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">x_list = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    x_sample = np.array([random.randint(<span class="number">1</span>, <span class="number">100</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(w_list))])</span><br><span class="line">    x_list.append(x_sample)</span><br></pre></td></tr></table></figure>

<p>这里，我们使用numpy，因为我们想利用numpy的点积函数轻松生成输出——y</p>
<p>说到y，我们来生成一个同样包含10个元素的y_list：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_list = []</span><br><span class="line"><span class="keyword">for</span> x_sample <span class="keyword">in</span> x_list:</span><br><span class="line">    y_temp = x_sample @ w_list </span><br><span class="line">    y_list.append(y_temp)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>我们的训练数据已准备就绪，无需下载任何内容，也无需使用DataLoader等，接下来，我们可以开始定义模型</p>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><p>我们的模型可能是世界上最简单的模型，即以下代码中定义的一个简单的线性点积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">​    <span class="built_in">super</span>().__init__()</span><br><span class="line">​    <span class="variable language_">self</span>.w = nn.Parameter(torch.randn(<span class="built_in">len</span>(w_list), dtype=torch.float32))</span><br><span class="line">​    <span class="built_in">print</span>(<span class="string">&quot;Initial weights:&quot;</span>, <span class="variable language_">self</span>.w)  <span class="comment"># Print initialized weights</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">​    <span class="keyword">return</span> <span class="variable language_">self</span>.w @ x</span><br></pre></td></tr></table></figure>

<p>在上述代码中，使用self.w &#x3D; nn.Parameter(torch.randn(len(w_list)))初始化了权重张量。</p>
<p>无需其他代码，我们的神经网络模型现已准备就绪，命名为——MyLinear</p>
<h2 id="准备训练模型"><a href="#准备训练模型" class="headerlink" title="准备训练模型"></a>准备训练模型</h2><p>我们需要像LLM那样初始化模型的随机权重</p>
<figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">model</span> <span class="operator">=</span> MyLinear()</span><br></pre></td></tr></table></figure>

<p>几乎所有神经网络模型的训练都遵循以下步骤：</p>
<ul>
<li>前向传播以预测结果</li>
<li>与真实值进行比较以获取损失值</li>
<li>反向传播梯度损失值</li>
<li>更新模型参数</li>
</ul>
<p>因此，在开始训练之前，我们需要定义一个<code>损失函数</code>和一个<code>优化器</code></p>
<p>损失函数<code>loss_fn</code>将根据预测结果和真实结果计算损失值，优化器将用于更新权重</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.00001</span>)</span><br></pre></td></tr></table></figure>

<p>lr代表学习率，这是最难设置的超参数之一，确定最佳学习率（lr）通常需要根据模型、数据集和问题的特性进行反复试验。然不过有一些策略和技术可以帮助估计一个合理的学习率</p>
<ul>
<li>从较小的学习率开始：一种常见做法是从较小的学习率（如0.001）开始，并根据观察到的收敛行为逐渐增加或减少它</li>
<li>学习率调度：可以使用学习率调度在训练过程中动态调整学习率。一种常见方法是阶梯衰减，即在固定数量的训练周期后降低学习率。另一种流行方法是指数衰减，即学习率随时间呈指数下降</li>
</ul>
<p>另外别忘了将输入和输出转换为torch张量对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_input = torch.tensor(x_list, dtype=torch.float32)</span><br><span class="line">y_output = torch.tensor(y_list, dtype=torch.float32)</span><br></pre></td></tr></table></figure>



<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>我们将训练周期数设置为100，这意味着我们将遍历训练数据100次</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练模型</span></span><br><span class="line">num_epochs = <span class="number">100</span>  <span class="comment"># 总训练周期</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(x_input):</span><br><span class="line">        <span class="comment"># 向前传播</span></span><br><span class="line">        y_pred = model(x)  <span class="comment"># Get prediction from model</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算损失值</span></span><br><span class="line">        loss = loss_fn(y_pred, y_output[i])  <span class="comment"># Compare prediction with true value</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 清除之前的缓存的梯度参数</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># Clear previous gradient information</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">        loss.backward()  <span class="comment"># Compute gradients for current parameters</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新模型参数</span></span><br><span class="line">        optimizer.step()  <span class="comment"># Update model parameters based on gradients</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 每十个epoch打印一次训练进度</span></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training completed&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>在上述代码中，我们可以看到两层循环，外层循环用于训练周期，内层循环用于遍历10组样本数据</p>
<p>我们有10组输入，每组输入有4个元素，即x_1, x_2, x_3, 和 x_4</p>
<p>正如我们在准备阶段所讨论的，第一步是使用模型预测一个结果：y_pred，然后，调用loss_fn来计算损失值</p>
<p>在将损失值反向传播之前，我们需要通过调用optimizer.zero_grad()来清除上一次的梯度值</p>
<p>最后，调用backward和optimizer.step()来更新模型参数</p>
<p>运行代码，我们将看到程序输出类似于以下内容：</p>
<figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="comment">[10/100]</span>, Loss: 218.3843</span><br><span class="line">Epoch <span class="comment">[20/100]</span>, Loss: 283.2002</span><br><span class="line">Epoch <span class="comment">[30/100]</span>, Loss: 116.5593</span><br><span class="line">Epoch <span class="comment">[40/100]</span>, Loss: 43.8340</span><br><span class="line">Epoch <span class="comment">[50/100]</span>, Loss: 16.3244</span><br><span class="line">Epoch <span class="comment">[60/100]</span>, Loss: 6.0721</span><br><span class="line">Epoch <span class="comment">[70/100]</span>, Loss: 2.2586</span><br><span class="line">Epoch <span class="comment">[80/100]</span>, Loss: 0.8400</span><br><span class="line">Epoch <span class="comment">[90/100]</span>, Loss: 0.3124</span><br><span class="line">Epoch <span class="comment">[100/100]</span>, Loss: 0.1162</span><br><span class="line">train done</span><br></pre></td></tr></table></figure>
<p>随着时间的推移，损失值在下降，这看起来很不错，我们在代码中添加以下功能：</p>
<ul>
<li><p>检查模型当前权重</p>
</li>
<li><p>增加权重数量(上述案例中我们只用了4个权重)</p>
</li>
<li><p>增加训练周期数和更新学习率——lr，</p>
</li>
</ul>
<p>以下是整理出的完整代码(无CUDA版) 同时改动了权重数量和训练周期</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ----------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 生成训练数据</span></span><br><span class="line"><span class="comment"># ----------------------------------------------------------</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义真实的权重向量（训练目标）</span></span><br><span class="line">w_list = np.array([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">5</span>, <span class="number">13</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成10个随机输入样本（每个样本长度与w_list相同）</span></span><br><span class="line">x_list = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    x_sample = np.array([random.randint(<span class="number">1</span>, <span class="number">100</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(w_list))])</span><br><span class="line">    x_list.append(x_sample)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算对应的目标输出（输入与真实权重的点积）</span></span><br><span class="line">y_list = []</span><br><span class="line"><span class="keyword">for</span> x_sample <span class="keyword">in</span> x_list:</span><br><span class="line">    y_temp = x_sample @ w_list  <span class="comment"># 矩阵乘法计算预测值</span></span><br><span class="line">    y_list.append(y_temp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 准备训练环境</span></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义自定义线性模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 初始化可学习参数（长度与w_list相同）</span></span><br><span class="line">        <span class="variable language_">self</span>.w = nn.Parameter(torch.randn(<span class="built_in">len</span>(w_list), dtype=torch.float32))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;初始权重:&quot;</span>, <span class="variable language_">self</span>.w)  <span class="comment"># 打印初始化的权重</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">        <span class="comment"># 前向传播：权重向量与输入向量的点积</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.w @ x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型（CPU模式）</span></span><br><span class="line">model = MyLinear()  <span class="comment"># 不使用GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数（均方误差）</span></span><br><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器（随机梯度下降，学习率0.00001）</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.00001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理（CPU模式）</span></span><br><span class="line">x_input = torch.tensor(x_list, dtype=torch.float32)  <span class="comment"># 输入数据转为张量</span></span><br><span class="line">y_output = torch.tensor(y_list, dtype=torch.float32)  <span class="comment"># 目标数据转为张量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 开始模型训练</span></span><br><span class="line"><span class="comment"># ------------------------------------------------------------</span></span><br><span class="line">num_epochs = <span class="number">200</span>  <span class="comment"># 总训练轮数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(x_input):</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        y_pred = model(x)  <span class="comment"># 获取模型预测值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = loss_fn(y_pred, y_output[i])  <span class="comment"># 比较预测值与真实值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 清除梯度缓存</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清除之前的梯度信息</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()  <span class="comment"># 计算当前参数的梯度</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 参数更新</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 根据梯度更新模型参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 每10轮打印一次训练进度</span></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;轮次 [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], 损失: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练完成&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最终权重:&quot;</span>, model.w.detach().numpy())</span><br></pre></td></tr></table></figure>

<p>输出如下：</p>
<p>初始模型权重：</p>
<figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">初始权重: Parameter containing:</span><br><span class="line">tensor([<span class="string">-0</span>.3772, <span class="string">-1</span>.7161,  0.2819, <span class="string">-0</span>.4298, <span class="string">-0</span>.8599, <span class="string">-0</span>.2690, <span class="string">-0</span>.5375],</span><br></pre></td></tr></table></figure>

<p>模型计算出的损失值和权重：</p>
<figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">轮次 <span class="comment">[10/200]</span>, 损失: 223.6982</span><br><span class="line">轮次 <span class="comment">[20/200]</span>, 损失: 3.3349</span><br><span class="line">轮次 <span class="comment">[30/200]</span>, 损失: 42.0250</span><br><span class="line">轮次 <span class="comment">[40/200]</span>, 损失: 41.3786</span><br><span class="line">轮次 <span class="comment">[50/200]</span>, 损失: 26.6852</span><br><span class="line">轮次 <span class="comment">[60/200]</span>, 损失: 14.8007</span><br><span class="line">轮次 <span class="comment">[70/200]</span>, 损失: 7.7069</span><br><span class="line">轮次 <span class="comment">[80/200]</span>, 损失: 3.9039</span><br><span class="line">轮次 <span class="comment">[90/200]</span>, 损失: 1.9543</span><br><span class="line">轮次 <span class="comment">[100/200]</span>, 损失: 0.9728</span><br><span class="line">轮次 <span class="comment">[110/200]</span>, 损失: 0.4838</span><br><span class="line">轮次 <span class="comment">[120/200]</span>, 损失: 0.2403</span><br><span class="line">轮次 <span class="comment">[130/200]</span>, 损失: 0.1195</span><br><span class="line">轮次 <span class="comment">[140/200]</span>, 损失: 0.0594</span><br><span class="line">轮次 <span class="comment">[150/200]</span>, 损失: 0.0295</span><br><span class="line">轮次 <span class="comment">[160/200]</span>, 损失: 0.0147</span><br><span class="line">轮次 <span class="comment">[170/200]</span>, 损失: 0.0073</span><br><span class="line">轮次 <span class="comment">[180/200]</span>, 损失: 0.0036</span><br><span class="line">轮次 <span class="comment">[190/200]</span>, 损失: 0.0018</span><br><span class="line">轮次 <span class="comment">[200/200]</span>, 损失: 0.0009</span><br><span class="line">训练完成</span><br><span class="line">最终权重: <span class="comment">[ 2.003787   3.0008602  3.9975853  6.9999294 10.997534   5.0006266</span></span><br><span class="line"><span class="comment"> 12.999097 ]</span></span><br></pre></td></tr></table></figure>

<p>我们代码中所定义的权重：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义真实的权重向量（训练目标）</span></span><br><span class="line"><span class="attribute">w_list</span> = np.array([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">5</span>, <span class="number">13</span>])</span><br></pre></td></tr></table></figure>

<p>从中我们可以看到我们的模型最终计算出的权重值非常[2,3,4,7,11,5,13]，由此得出模型已成功训练并找到了正确的权重值！</p>

    </div>
    
    <!-- Post Footer -->
    <footer class="post-footer">
        
            <div class="post-updated">
                <small>Last updated: July 24, 2025</small>
            </div>
        
        
        <!-- Post Navigation -->
        <nav class="post-nav">
            
                <div class="post-nav-item post-nav-prev">
                    <a href="/2025/07/21/%E6%94%BB%E9%98%B2%E4%B8%96%E7%95%8CCrypto-baigeiRSA/" class="post-nav-link">
                        <span class="post-nav-label">← Previous</span>
                        <span class="post-nav-title">攻防世界Crypto-baigeiRSA</span>
                    </a>
                </div>
            
            
            
                <div class="post-nav-item post-nav-next">
                    <a href="/2025/07/18/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%9701-%E6%A5%94%E5%AD%90/" class="post-nav-link">
                        <span class="post-nav-label">Next →</span>
                        <span class="post-nav-title">动手学深度学习系列01-楔子</span>
                    </a>
                </div>
            
        </nav>
    </footer>
</article>

<!-- Comments Section -->

        </main>

        <!-- Footer -->
        <footer class="footer">
            <div class="footer-content">
                <p>&copy; 2025 zer0ptr. All rights reserved.</p>
                
            </div>
        </footer>
    </div>

    <!-- 按需加载Highlight.js JavaScript -->
    
        <script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
        <script>
            // 延迟加载highlight.js以提高性能
            function loadHighlight() {
                if (typeof hljs !== 'undefined') {
                    hljs.highlightAll();
                } else {
                    // 如果hljs还未加载，等待一段时间后重试
                    setTimeout(loadHighlight, 100);
                }
            }
            
            document.addEventListener('DOMContentLoaded', loadHighlight);
            
            // 如果DOM已经加载完成，立即执行
            if (document.readyState !== 'loading') {
                loadHighlight();
            }
        </script>
    
    
    <!-- JavaScript -->
    <script defer src="/js/search.js"></script>
    <script defer src="/js/script.js"></script>
</body>
</html>