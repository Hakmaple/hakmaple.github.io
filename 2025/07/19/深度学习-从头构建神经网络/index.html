<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>深度学习：从头构建神经网络 | zer0ptr&#39;s Blog</title>
  <meta name="author" content="zer0ptr">
  
  <meta name="description" content="Description本文旨在使用PyTorch构建并训练一个最简单的神经网络，无需添加任何花哨的层或依赖包。
该模型将足够简单，大家都能使用CPU或GPU来构建和训练。
这个模型虽然简单，但包含了当前诸如LLM和Stable Diffusions等大型模型所拥有的所有基本元素。
准备数据假设我们要训练一个具有四个权重并能输出一个数字结果的模型，如下所示：
$ y &amp;#x3D; w_1 * x_1 + w_2 * x_2 + w_3 * x_3 + w_4 * x_4 $
我们先生成以下训练数据，假设权重值为 [2,3,4,7]
12import numpy as npw_list = np.array([2,3,4,7])

我们的模型将会用于预测权重列表，因此再生成一些训练数据后我们假装我们不知道这些权重的值"> 
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="深度学习：从头构建神经网络"/>
  <meta property="og:site_name" content="zer0ptr&#39;s Blog"/>

  
    <meta property="og:image" content=""/>
  

  
    <link rel="alternative" href="/atom.xml" title="zer0ptr&#39;s Blog" type="application/atom+xml">
  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/prism.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-70812759-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-70812759-1');
</script>






<meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">zer0ptr&#39;s Blog</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class=""></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class=""></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class=""></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class=""></i>About
			</a>
		  </li>
		  
		  <li>
			<a href="/atom.xml" title="Subscribe me.">
			  <i class=""></i>RSS
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> 深度学习：从头构建神经网络</h1>
		</div>
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>本文旨在使用PyTorch构建并训练一个最简单的神经网络，无需添加任何花哨的层或依赖包。</p>
<p>该模型将足够简单，大家都能使用CPU或GPU来构建和训练。</p>
<p>这个模型虽然简单，但包含了当前诸如LLM和Stable Diffusions等大型模型所拥有的所有基本元素。</p>
<h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>假设我们要训练一个具有四个权重并能输出一个数字结果的模型，如下所示：</p>
<p>$ y &#x3D; w_1 * x_1 + w_2 * x_2 + w_3 * x_3 + w_4 * x_4 $</p>
<p>我们先生成以下训练数据，假设权重值为 [2,3,4,7]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">w_list = np.array([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">7</span>])</span><br></pre></td></tr></table></figure>

<p>我们的模型将会用于预测权重列表，因此再生成一些训练数据后我们假装我们不知道这些权重的值</p>
<p>之后我们创建10组输入样本数据——x_sample，每组x_sample是一个包含4个元素的数组，与权重的长度相同</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generate 10 random input samples (each with same length as w_list)</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">x_list = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    x_sample = np.array([random.randint(<span class="number">1</span>, <span class="number">100</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(w_list))])</span><br><span class="line">    x_list.append(x_sample)</span><br></pre></td></tr></table></figure>

<p>这里，我们使用numpy，因为我们想利用numpy的点积函数轻松生成输出——y</p>
<p>说到y，我们来生成一个同样包含10个元素的y_list：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_list = []</span><br><span class="line"><span class="keyword">for</span> x_sample <span class="keyword">in</span> x_list:</span><br><span class="line">    y_temp = x_sample @ w_list </span><br><span class="line">    y_list.append(y_temp)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>我们的训练数据已准备就绪，无需下载任何内容，也无需使用DataLoader等，接下来，我们可以开始定义模型</p>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><p>我们的模型可能是世界上最简单的模型，即以下代码中定义的一个简单的线性点积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">​    <span class="built_in">super</span>().__init__()</span><br><span class="line">​    <span class="variable language_">self</span>.w = nn.Parameter(torch.randn(<span class="built_in">len</span>(w_list), dtype=torch.float32))</span><br><span class="line">​    <span class="built_in">print</span>(<span class="string">&quot;Initial weights:&quot;</span>, <span class="variable language_">self</span>.w)  <span class="comment"># Print initialized weights</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">​    <span class="keyword">return</span> <span class="variable language_">self</span>.w @ x</span><br></pre></td></tr></table></figure>

<p>在上述代码中，使用self.w &#x3D; nn.Parameter(torch.randn(len(w_list)))初始化了权重张量。</p>
<p>无需其他代码，我们的神经网络模型现已准备就绪，命名为——MyLinear</p>
<h2 id="准备训练模型"><a href="#准备训练模型" class="headerlink" title="准备训练模型"></a>准备训练模型</h2><p>我们需要像LLM那样初始化模型的随机权重</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = MyLinear()</span><br></pre></td></tr></table></figure>

<p>几乎所有神经网络模型的训练都遵循以下步骤：</p>
<ul>
<li>前向传播以预测结果</li>
<li>与真实值进行比较以获取损失值</li>
<li>反向传播梯度损失值</li>
<li>更新模型参数</li>
</ul>
<p>因此，在开始训练之前，我们需要定义一个<code>损失函数</code>和一个<code>优化器</code></p>
<p>损失函数<code>loss_fn</code>将根据预测结果和真实结果计算损失值，优化器将用于更新权重</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.00001</span>)</span><br></pre></td></tr></table></figure>

<p>lr代表学习率，这是最难设置的超参数之一，确定最佳学习率（lr）通常需要根据模型、数据集和问题的特性进行反复试验。然不过有一些策略和技术可以帮助估计一个合理的学习率</p>
<ul>
<li>从较小的学习率开始：一种常见做法是从较小的学习率（如0.001）开始，并根据观察到的收敛行为逐渐增加或减少它</li>
<li>学习率调度：可以使用学习率调度在训练过程中动态调整学习率。一种常见方法是阶梯衰减，即在固定数量的训练周期后降低学习率。另一种流行方法是指数衰减，即学习率随时间呈指数下降</li>
</ul>
<p>另外别忘了将输入和输出转换为torch张量对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_input = torch.tensor(x_list, dtype=torch.float32)</span><br><span class="line">y_output = torch.tensor(y_list, dtype=torch.float32)</span><br></pre></td></tr></table></figure>



<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>我们将训练周期数设置为100，这意味着我们将遍历训练数据100次</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 开始训练模型</span><br><span class="line">num_epochs = 100  # 总训练周期</span><br><span class="line">for epoch in range(num_epochs):</span><br><span class="line">    for i, x in enumerate(x_input):</span><br><span class="line">        # 向前传播</span><br><span class="line">        y_pred = model(x)  # Get prediction from model</span><br><span class="line">        </span><br><span class="line">        # 计算损失值</span><br><span class="line">        loss = loss_fn(y_pred, y_output[i])  # Compare prediction with true value</span><br><span class="line">        </span><br><span class="line">        # 清除之前的缓存的梯度参数</span><br><span class="line">        optimizer.zero_grad()  # Clear previous gradient information</span><br><span class="line">        </span><br><span class="line">        # 反向传播计算梯度</span><br><span class="line">        loss.backward()  # Compute gradients for current parameters</span><br><span class="line">        </span><br><span class="line">        # 更新模型参数</span><br><span class="line">        optimizer.step()  # Update model parameters based on gradients</span><br><span class="line">    </span><br><span class="line">    # 每十个epoch打印一次训练进度</span><br><span class="line">    if (epoch+1) % 10 == 0:</span><br><span class="line">        print(f&#x27;Epoch [&#123;epoch+1&#125;/&#123;num_epochs&#125;], Loss: &#123;loss.item():.4f&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">print(&quot;Training completed&quot;)</span><br></pre></td></tr></table></figure>

<p>在上述代码中，我们可以看到两层循环，外层循环用于训练周期，内层循环用于遍历10组样本数据</p>
<p>我们有10组输入，每组输入有4个元素，即x_1, x_2, x_3, 和 x_4</p>
<p>正如我们在准备阶段所讨论的，第一步是使用模型预测一个结果：y_pred，然后，调用loss_fn来计算损失值</p>
<p>在将损失值反向传播之前，我们需要通过调用optimizer.zero_grad()来清除上一次的梯度值</p>
<p>最后，调用backward和optimizer.step()来更新模型参数</p>
<p>运行代码，我们将看到程序输出类似于以下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Epoch [10/100], Loss: 218.3843</span><br><span class="line">Epoch [20/100], Loss: 283.2002</span><br><span class="line">Epoch [30/100], Loss: 116.5593</span><br><span class="line">Epoch [40/100], Loss: 43.8340</span><br><span class="line">Epoch [50/100], Loss: 16.3244</span><br><span class="line">Epoch [60/100], Loss: 6.0721</span><br><span class="line">Epoch [70/100], Loss: 2.2586</span><br><span class="line">Epoch [80/100], Loss: 0.8400</span><br><span class="line">Epoch [90/100], Loss: 0.3124</span><br><span class="line">Epoch [100/100], Loss: 0.1162</span><br><span class="line">train done</span><br></pre></td></tr></table></figure>
<p>随着时间的推移，损失值在下降，这看起来很不错，我们在代码中添加以下功能：</p>
<ul>
<li><p>检查模型当前权重</p>
</li>
<li><p>增加权重数量(上述案例中我们只用了4个权重)</p>
</li>
<li><p>增加训练周期数和更新学习率——lr，</p>
</li>
</ul>
<p>以下是整理出的完整代码(无CUDA版) 同时改动了权重数量和训练周期</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ----------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 生成训练数据</span></span><br><span class="line"><span class="comment"># ----------------------------------------------------------</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义真实的权重向量（训练目标）</span></span><br><span class="line">w_list = np.array([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">5</span>, <span class="number">13</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成10个随机输入样本（每个样本长度与w_list相同）</span></span><br><span class="line">x_list = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    x_sample = np.array([random.randint(<span class="number">1</span>, <span class="number">100</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(w_list))])</span><br><span class="line">    x_list.append(x_sample)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算对应的目标输出（输入与真实权重的点积）</span></span><br><span class="line">y_list = []</span><br><span class="line"><span class="keyword">for</span> x_sample <span class="keyword">in</span> x_list:</span><br><span class="line">    y_temp = x_sample @ w_list  <span class="comment"># 矩阵乘法计算预测值</span></span><br><span class="line">    y_list.append(y_temp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 准备训练环境</span></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义自定义线性模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 初始化可学习参数（长度与w_list相同）</span></span><br><span class="line">        <span class="variable language_">self</span>.w = nn.Parameter(torch.randn(<span class="built_in">len</span>(w_list), dtype=torch.float32))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;初始权重:&quot;</span>, <span class="variable language_">self</span>.w)  <span class="comment"># 打印初始化的权重</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">        <span class="comment"># 前向传播：权重向量与输入向量的点积</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.w @ x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型（CPU模式）</span></span><br><span class="line">model = MyLinear()  <span class="comment"># 不使用GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数（均方误差）</span></span><br><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器（随机梯度下降，学习率0.00001）</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.00001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理（CPU模式）</span></span><br><span class="line">x_input = torch.tensor(x_list, dtype=torch.float32)  <span class="comment"># 输入数据转为张量</span></span><br><span class="line">y_output = torch.tensor(y_list, dtype=torch.float32)  <span class="comment"># 目标数据转为张量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 开始模型训练</span></span><br><span class="line"><span class="comment"># ------------------------------------------------------------</span></span><br><span class="line">num_epochs = <span class="number">200</span>  <span class="comment"># 总训练轮数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(x_input):</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        y_pred = model(x)  <span class="comment"># 获取模型预测值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = loss_fn(y_pred, y_output[i])  <span class="comment"># 比较预测值与真实值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 清除梯度缓存</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清除之前的梯度信息</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()  <span class="comment"># 计算当前参数的梯度</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 参数更新</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 根据梯度更新模型参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 每10轮打印一次训练进度</span></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;轮次 [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], 损失: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练完成&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最终权重:&quot;</span>, model.w.detach().numpy())</span><br></pre></td></tr></table></figure>

<p>输出如下：</p>
<p>初始模型权重：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">初始权重: Parameter containing:</span><br><span class="line">tensor([-0.3772, -1.7161,  0.2819, -0.4298, -0.8599, -0.2690, -0.5375],</span><br></pre></td></tr></table></figure>

<p>模型计算出的损失值和权重：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">轮次 [10/200], 损失: 223.6982</span><br><span class="line">轮次 [20/200], 损失: 3.3349</span><br><span class="line">轮次 [30/200], 损失: 42.0250</span><br><span class="line">轮次 [40/200], 损失: 41.3786</span><br><span class="line">轮次 [50/200], 损失: 26.6852</span><br><span class="line">轮次 [60/200], 损失: 14.8007</span><br><span class="line">轮次 [70/200], 损失: 7.7069</span><br><span class="line">轮次 [80/200], 损失: 3.9039</span><br><span class="line">轮次 [90/200], 损失: 1.9543</span><br><span class="line">轮次 [100/200], 损失: 0.9728</span><br><span class="line">轮次 [110/200], 损失: 0.4838</span><br><span class="line">轮次 [120/200], 损失: 0.2403</span><br><span class="line">轮次 [130/200], 损失: 0.1195</span><br><span class="line">轮次 [140/200], 损失: 0.0594</span><br><span class="line">轮次 [150/200], 损失: 0.0295</span><br><span class="line">轮次 [160/200], 损失: 0.0147</span><br><span class="line">轮次 [170/200], 损失: 0.0073</span><br><span class="line">轮次 [180/200], 损失: 0.0036</span><br><span class="line">轮次 [190/200], 损失: 0.0018</span><br><span class="line">轮次 [200/200], 损失: 0.0009</span><br><span class="line">训练完成</span><br><span class="line">最终权重: [ 2.003787   3.0008602  3.9975853  6.9999294 10.997534   5.0006266</span><br><span class="line"> 12.999097 ]</span><br></pre></td></tr></table></figure>

<p>我们代码中所定义的权重：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 定义真实的权重向量（训练目标）</span><br><span class="line">w_list = np.array([2, 3, 4, 7, 11, 5, 13])</span><br></pre></td></tr></table></figure>

<p>从中我们可以看到我们的模型最终计算出的权重值非常[2,3,4,7,11,5,13]，由此得出模型已成功训练并找到了正确的权重值！</p>

	  <div class="article-footer-copyright">

    本博客采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议(CC BY-NC-SA 4.0) 发布.</a>
</div>

	</div>

	
	
	<div>
  	<center>

	<div class="pagination">

    
    
    <a href="/2025/07/21/攻防世界Crypto-baigeiRSA/" type="button" class="btn btn-default"><i
                class="fa fa-arrow-circle-o-left"></i> Prev</a>
    

    <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/2025/07/18/动手学深度学习系列01-楔子/" type="button" class="btn btn-default ">Next<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>


    </center>
	</div>
	
	<!-- comment -->
	<!--
<section id="comment">
    <h2 class="title">Comments</h2>

    
</section>

-->
	
	
	
	</div> <!-- col-md-9/col-md-12 -->


	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2025-07-19 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/AI/">AI<span>3</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/AI/">AI<span>3</span></a></li> <li><a href="/tags/Deep-Learning/">Deep Learning<span>1</span></a></li>
    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

		

	</div>
	
		

</div><!-- row -->

<!--
 -->



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  
  &copy; 2025 zer0ptr's Blog
  
      powered by <a href="http://hexo.io/" target="_blank">Hexo</a>.Theme <a href="https://github.com/Ares-X/hexo-theme-freemind.bithack" target="_blank">freemind.bithack</a>  
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

</body>
   </html>
